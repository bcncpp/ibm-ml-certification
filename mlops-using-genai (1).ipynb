{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## A seamless MLOps workflow, powered by Generative AI","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:04.874359Z","iopub.execute_input":"2025-04-10T15:19:04.876572Z","iopub.status.idle":"2025-04-10T15:19:04.900701Z","shell.execute_reply.started":"2025-04-10T15:19:04.876526Z","shell.execute_reply":"2025-04-10T15:19:04.899560Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"#  1. Setup: Install and Import\n!pip install -q langchain langchain-google-genai google-generativeai pandas scikit-learn matplotlib seaborn pyyaml rich","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:04.902471Z","iopub.execute_input":"2025-04-10T15:19:04.902773Z","iopub.status.idle":"2025-04-10T15:19:09.463598Z","shell.execute_reply.started":"2025-04-10T15:19:04.902745Z","shell.execute_reply":"2025-04-10T15:19:09.462077Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"pip install -q -U google-genai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:09.465673Z","iopub.execute_input":"2025-04-10T15:19:09.466133Z","iopub.status.idle":"2025-04-10T15:19:13.779941Z","shell.execute_reply.started":"2025-04-10T15:19:09.466102Z","shell.execute_reply":"2025-04-10T15:19:13.778733Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport google.generativeai as genai\nimport os\nimport warnings\nimport requests\nimport json\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.781911Z","iopub.execute_input":"2025-04-10T15:19:13.782316Z","iopub.status.idle":"2025-04-10T15:19:13.788534Z","shell.execute_reply.started":"2025-04-10T15:19:13.782277Z","shell.execute_reply":"2025-04-10T15:19:13.787407Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# -------------------- CONFIG --------------------\nGEMINI_API_KEY = \"Your-GEMINI-API_Key\"\nGEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\"\nHEADERS = {\"Content-Type\": \"application/json\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.789544Z","iopub.execute_input":"2025-04-10T15:19:13.789804Z","iopub.status.idle":"2025-04-10T15:19:13.808096Z","shell.execute_reply.started":"2025-04-10T15:19:13.789779Z","shell.execute_reply":"2025-04-10T15:19:13.807032Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# -------------------- GEMINI HELPER --------------------\ndef query_gemini(prompt):\n    payload = {\n        \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n    }\n    response = requests.post(\n        f\"{GEMINI_API_URL}?key={GEMINI_API_KEY}\",\n        headers=HEADERS,\n        data=json.dumps(payload)\n    )\n    return response.json()['candidates'][0]['content']['parts'][0]['text']\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.809423Z","iopub.execute_input":"2025-04-10T15:19:13.809780Z","iopub.status.idle":"2025-04-10T15:19:13.827144Z","shell.execute_reply.started":"2025-04-10T15:19:13.809746Z","shell.execute_reply":"2025-04-10T15:19:13.826167Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"#  Step 1: Load Data\ndef load_data():\n    df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.828157Z","iopub.execute_input":"2025-04-10T15:19:13.828495Z","iopub.status.idle":"2025-04-10T15:19:13.847376Z","shell.execute_reply.started":"2025-04-10T15:19:13.828458Z","shell.execute_reply":"2025-04-10T15:19:13.846292Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# üßπ Step 2: Preprocess Data\n\ndef preprocess_data(df):\n    #  Prompt to Gemini: Ask for preprocessing steps without reading CSV\n    prompt = \"\"\"\nYou are a data scientist. Clean the Titanic dataset with steps like:\n\n1. Handle missing values\n2. Encode categorical variables\n3. Drop irrelevant columns\n\nThe DataFrame is already loaded as df. DO NOT use read_csv(). Output Python code ONLY. No markdown or explanation.\n\"\"\"\n    # Get preprocessing suggestion from Gemini\n    suggestion = query_gemini(prompt)\n\n    # Clean Gemini's response ‚Äî remove backticks and extra formatting\n    clean_code = suggestion.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n\n    # Extra safeguard: Remove any accidental `read_csv()` lines\n    clean_code = \"\\n\".join([\n        line for line in clean_code.splitlines()\n        if 'read_csv' not in line\n    ])\n\n    print(\"Preprocessing Code:\\n\", clean_code)\n\n    # Create a new execution environment with df as context\n    exec_globals = {'df': df.copy()}\n\n    #  Dynamically execute the Gemini-generated code\n    exec(clean_code, exec_globals)\n    print(\"üìä Columns after preprocessing:\", exec_globals['df'].shape)\n\n\n    # Return the cleaned DataFrame\n    return exec_globals['df']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.850927Z","iopub.execute_input":"2025-04-10T15:19:13.851328Z","iopub.status.idle":"2025-04-10T15:19:13.867511Z","shell.execute_reply.started":"2025-04-10T15:19:13.851295Z","shell.execute_reply":"2025-04-10T15:19:13.866473Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def train_model(X_train, y_train):\n    prompt = \"\"\"\nYou are a machine learning engineer. Train a RandomForestClassifier using the variables X_train and y_train.\nDo NOT generate synthetic data. Use X_train and y_train directly. Name the model as `model`.\nReturn ONLY the training code.\n\"\"\"\n    suggestion = query_gemini(prompt)\n\n    # Clean Gemini's output\n    clean_code = suggestion.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n\n    # Strip out any code that tries to redefine data\n    forbidden_phrases = [\"make_classification\", \"X =\", \"y =\", \"train_test_split\"]\n    clean_code = \"\\n\".join([\n        line for line in clean_code.splitlines()\n        if not any(p in line for p in forbidden_phrases)\n    ])\n\n    # Force consistent model variable name\n    clean_code = clean_code.replace(\"rf_classifier\", \"model\").replace(\"clf\", \"model\")\n\n    print(\" Training Code:\\n\", clean_code)\n\n    exec_globals = {'X_train': X_train, 'y_train': y_train}\n    exec(clean_code, exec_globals)\n\n    if 'model' not in exec_globals:\n        raise ValueError(\" ERROR: `model` was not defined in the training code.\")\n\n    return exec_globals['model']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.868643Z","iopub.execute_input":"2025-04-10T15:19:13.868970Z","iopub.status.idle":"2025-04-10T15:19:13.879082Z","shell.execute_reply.started":"2025-04-10T15:19:13.868941Z","shell.execute_reply":"2025-04-10T15:19:13.878077Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def evaluate_model(model, X_test, y_test):\n    from sklearn.metrics import accuracy_score, classification_report\n\n    # üéØ Predict on test data\n    y_pred = model.predict(X_test)\n\n    # üßÆ Compute metrics\n    acc = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    # üìù Log evaluation\n    print(\"‚úÖ Accuracy:\", acc)\n    print(\"üìä Classification Report:\\n\", report)\n\n    # üìÅ Optional: Save results to output folder\n    with open(\"evaluation_log.txt\", \"w\") as f:\n        f.write(f\"Accuracy: {acc}\\n\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(report)\n\n    # ü§ñ Gemini-based evaluation summary (optional, toggle on/off)\n    enable_gemini_summary = True\n    if enable_gemini_summary:\n        prompt = f\"\"\"\nYou are a machine learning expert. Analyze the following classification report and accuracy score.\n\nAccuracy: {acc}\n\nReport:\n{report}\n\nExplain in simple terms how well the model is performing and suggest ways to improve it if needed.\n\"\"\"\n        gemini_feedback = query_gemini(prompt)\n        print(\"üí° Gemini Summary:\\n\", gemini_feedback)\n\n        with open(\"gemini_evaluation_summary.txt\", \"w\") as f:\n            f.write(gemini_feedback)\n\n    return acc, report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.880289Z","iopub.execute_input":"2025-04-10T15:19:13.880671Z","iopub.status.idle":"2025-04-10T15:19:13.898605Z","shell.execute_reply.started":"2025-04-10T15:19:13.880642Z","shell.execute_reply":"2025-04-10T15:19:13.897654Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def generate_report(acc, report, save_to_file=True, verbose=True):\n    \"\"\"\n    Generates a human-readable evaluation report using Gemini 1.5 Flash.\n\n    Args:\n        acc (float): Accuracy of the model.\n        report (str): Classification report from sklearn.\n        save_to_file (bool): If True, saves the summary to a text file.\n        verbose (bool): If True, prints the summary.\n\n    Returns:\n        str: Gemini-generated summary report.\n    \"\"\"\n    prompt = f\"\"\"\nYou are a machine learning evaluator. Here is a model performance summary:\n\nAccuracy: {acc}\n\nClassification Report:\n{report}\n\nWrite a professional, human-readable evaluation report. Include insights into precision, recall, and F1-score, and suggest potential improvements if applicable.\n\"\"\"\n\n    gemini_summary = query_gemini(prompt)\n\n    if verbose:\n        print(\"üìã Gemini Evaluation Summary:\\n\")\n        print(gemini_summary)\n\n    if save_to_file:\n        with open(\"final_model_summary.txt\", \"w\") as f:\n            f.write(\"üéØ Accuracy: \" + str(acc) + \"\\n\\n\")\n            f.write(\"üìä Classification Report:\\n\" + report + \"\\n\\n\")\n            f.write(\"üß† Gemini Summary:\\n\" + gemini_summary)\n\n    return gemini_summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.899552Z","iopub.execute_input":"2025-04-10T15:19:13.899805Z","iopub.status.idle":"2025-04-10T15:19:13.912329Z","shell.execute_reply.started":"2025-04-10T15:19:13.899783Z","shell.execute_reply":"2025-04-10T15:19:13.911380Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import datetime\nimport joblib\n\ndef run_pipeline():\n    output_dir = \"/kaggle/working/output\"\n    os.makedirs(output_dir, exist_ok=True)\n    log_path = os.path.join(output_dir, \"mlops_log.txt\")\n\n    try:\n        with open(log_path, \"w\") as log_file:\n            log = lambda msg: log_file.write(f\"[{datetime.datetime.now()}] {msg}\\n\")\n\n            log(\"üöÄ Starting MLOps Pipeline\")\n\n            df = load_data()\n            log(\"‚úÖ Loaded data.\")\n\n            df_clean = preprocess_data(df)\n            log(\"üßπ Preprocessed data.\")\n\n            X = df_clean.drop(\"Survived\", axis=1)\n            y = df_clean[\"Survived\"]\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n            print(\"üîé Training data shape:\", X_train.shape)\n\n\n            model = train_model(X_train, y_train)\n            log(\"ü§ñ Trained model.\")\n\n            acc, eval_report = evaluate_model(model, X_test, y_test)\n            log(f\"üß™ Evaluation Accuracy: {acc}\")\n            log(eval_report)\n\n            summary = generate_report(acc, eval_report)\n            log(\"\\nüìã Gemini-generated Summary:\")\n            log(summary)\n\n            # Optional: Save model\n            model_path = os.path.join(output_dir, \"random_forest_model.pkl\")\n            joblib.dump(model, model_path)\n            log(f\"üíæ Model saved to: {model_path}\")\n\n            print(\"‚úÖ MLOps Pipeline Complete. Check output folder for logs, summary, and model.\")\n\n    except Exception as e:\n        with open(log_path, \"a\") as log_file:\n            log_file.write(f\"[{datetime.datetime.now()}]  ERROR: {str(e)}\\n\")\n        print(\" Pipeline failed. Check logs for details.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.913405Z","iopub.execute_input":"2025-04-10T15:19:13.913731Z","iopub.status.idle":"2025-04-10T15:19:13.926201Z","shell.execute_reply.started":"2025-04-10T15:19:13.913703Z","shell.execute_reply":"2025-04-10T15:19:13.925050Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T15:19:13.927377Z","iopub.execute_input":"2025-04-10T15:19:13.927654Z","iopub.status.idle":"2025-04-10T15:19:28.667073Z","shell.execute_reply.started":"2025-04-10T15:19:13.927628Z","shell.execute_reply":"2025-04-10T15:19:28.665950Z"}},"outputs":[{"name":"stdout","text":"Preprocessing Code:\n import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Handle missing values\ndf['Age'].fillna(df['Age'].median(), inplace=True)\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\ndf.dropna(subset=['Fare'], inplace=True)\n\n# Encode categorical variables\nle = LabelEncoder()\nfor col in ['Sex', 'Embarked']:\n    df[col] = le.fit_transform(df[col])\n\n# Drop irrelevant columns\ndf.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\nüìä Columns after preprocessing: (891, 8)\nüîé Training data shape: (712, 7)\n Training Code:\n from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n‚úÖ Accuracy: 0.8212290502793296\nüìä Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.85      0.85      0.85       105\n           1       0.78      0.78      0.78        74\n\n    accuracy                           0.82       179\n   macro avg       0.82      0.82      0.82       179\nweighted avg       0.82      0.82      0.82       179\n\nüí° Gemini Summary:\n The model is performing reasonably well, with an overall accuracy of 82%.  This means it correctly classified 82% of the samples in the test set.\n\nLet's break down the classification report:\n\n* **Accuracy:**  A general measure of correctness.  82% is decent but not exceptional.\n\n* **Precision (for each class):**  Precision answers \"Of all the instances *predicted* as class X, what proportion were actually class X?\".  For class 0, the precision is 85%, meaning 85% of the instances predicted as class 0 were actually class 0.  For class 1, it's 78%.  A lower precision for class 1 suggests more false positives (incorrectly predicting class 1).\n\n* **Recall (for each class):** Recall answers \"Of all the instances that *actually are* class X, what proportion did the model correctly predict?\". For class 0, the recall is 85%, meaning the model correctly identified 85% of the actual class 0 instances. For class 1, it's 78%. A lower recall for class 1 suggests more false negatives (incorrectly predicting class 0 when it was actually class 1).\n\n* **F1-score (for each class):** The F1-score is the harmonic mean of precision and recall.  It provides a balanced measure considering both false positives and false negatives.  It's similar to precision and recall for both classes in this case.\n\n* **Support:** This indicates the number of instances of each class in the test set (105 for class 0 and 74 for class 1).  The class imbalance isn't extreme, but it's worth noting.\n\n\n**Areas for Improvement:**\n\nThe model's performance is relatively balanced across classes, but there's room for improvement:\n\n1. **Address Class Imbalance (Slightly):** While not severely imbalanced, having slightly more instances of class 0 might slightly skew the results.  Techniques like oversampling (creating more instances of class 1) or undersampling (reducing instances of class 0) could be explored, but only if the imbalance significantly impacts the results.  Start with other methods first.\n\n2. **Feature Engineering:**  Explore adding or transforming existing features.  Are there additional variables that could improve the model's ability to distinguish between classes?  Feature selection (removing irrelevant or redundant features) could also be beneficial.\n\n3. **Hyperparameter Tuning:** The model's hyperparameters (settings that control the learning process) might need adjustment.  Experiment with different hyperparameter values using techniques like grid search or randomized search to find an optimal configuration.\n\n4. **Try Different Models:** Consider experimenting with different classification algorithms (e.g., Support Vector Machines, Random Forests, Gradient Boosting Machines) to see if any perform better.\n\n5. **Data Augmentation (If applicable):** If your data allows, creating synthetic data points that are similar to your existing data could help improve the model's robustness and generalization ability.  This is most relevant with image or audio data.\n\n\nBefore implementing any of these, carefully analyze the data for errors or inconsistencies.  A thorough understanding of the data and the problem you are trying to solve is crucial for effective model improvement.\n\nüìã Gemini Evaluation Summary:\n\n## Model Performance Evaluation Report\n\nThis report evaluates the performance of a binary classification model based on provided metrics.  The model achieves an overall accuracy of 82.12%, indicating a reasonable level of correctness in classifying instances. However, a more detailed analysis of precision, recall, and F1-score reveals nuances in its performance across classes.\n\n**Key Metrics:**\n\n* **Accuracy (0.8212):**  The overall proportion of correctly classified instances. This suggests the model performs reasonably well but leaves room for improvement.\n\n* **Precision:**  Precision measures the accuracy of positive predictions.  For class 0, precision is 0.85, meaning that 85% of instances predicted as class 0 are actually class 0. For class 1, precision is 0.78, suggesting a slightly lower accuracy in predicting positive instances of this class.\n\n* **Recall:** Recall measures the model's ability to identify all positive instances. For class 0, recall is 0.85, indicating the model correctly identifies 85% of the actual class 0 instances. Similarly, recall for class 1 is 0.78, meaning the model correctly identifies 78% of actual class 1 instances.\n\n* **F1-score:** The F1-score is the harmonic mean of precision and recall, providing a balanced measure of a model's performance.  Both classes exhibit an F1-score of 0.85 and 0.78 respectively, mirroring the precision and recall values.  This indicates a relatively balanced performance for class 0, while class 1 shows slightly lower performance.\n\n\n**Insights and Potential Improvements:**\n\nThe model demonstrates a reasonably good performance overall, but the slightly lower precision and recall for class 1 (compared to class 0) suggests a potential area for improvement. This imbalance might stem from several factors:\n\n* **Class Imbalance:** The dataset may have an inherent class imbalance (105 instances of class 0 vs 74 of class 1).  This can skew the model's performance towards the majority class.  Addressing this might involve techniques like oversampling the minority class (class 1), undersampling the majority class, or using cost-sensitive learning.\n\n* **Feature Engineering:**  Exploring additional or modified features could improve the model's ability to discriminate between the classes. This could involve creating new features from existing ones or using feature selection techniques to identify the most relevant features.\n\n* **Model Selection:**  Experimenting with different algorithms (e.g., Support Vector Machines, Random Forests, or different neural network architectures) could yield better results.  Hyperparameter tuning within the current model is also crucial.\n\n* **Data Quality:**  Investigating the quality of the training data is crucial.  Noise, missing values, or outliers could negatively impact the model's performance. Data cleaning and preprocessing steps should be carefully reviewed.\n\n\n**Recommendations:**\n\n1. **Investigate class imbalance:** Analyze the impact of class imbalance and apply appropriate resampling techniques.\n2. **Perform feature engineering:** Explore creating new features that might improve class separability.\n3. **Hyperparameter tuning:** Optimize the model's hyperparameters to improve performance, focusing particularly on class 1.\n4. **Evaluate different models:**  Explore alternative algorithms to determine if a different model architecture better suits the data.\n5. **Assess data quality:** Thoroughly review data cleaning and preprocessing to ensure data quality.\n\n\nBy addressing these potential areas for improvement, the model's performance, particularly for class 1, can likely be enhanced.  Further analysis, incorporating techniques mentioned above, is recommended to improve the overall predictive capability of the model.\n\n‚úÖ MLOps Pipeline Complete. Check output folder for logs, summary, and model.\n","output_type":"stream"}],"execution_count":52}]}